@article{Munos2004ASO,
  title={A Study of Reinforcement Learning in the Continuous Case by the Means of Viscosity Solutions},
  author={R. Munos},
  journal={Machine Learning},
  year={2004},
  volume={40},
  pages={265-299}
}
@inproceedings{Rowland48495,
title	= {Statistics and Samples in Distributional Reinforcement Learning},
author	= {Mark Rowland and Robert Dadashi and Saurabh Kumar and Remi Munos and Marc Bellemare and Will Dabney},
year	= {2019},
URL	= {http://proceedings.mlr.press/v97/rowland19a/rowland19a.pdf},
booktitle	= {Proceedings of the 36th International Conference on Machine Learning},
pages	= {5528--5536}
}
@article{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {JAX: composable transformations of Python+NumPy programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}
@inproceedings{Martn2016ReadMD,
  title={Read Markov Decision Processes Discrete Stochastic Dynamic Programming Markov Decision Processes Discrete Stochastic Dynamic Programming},
  author={Mart{\'i}n and Puterman},
  year={2016}
}
@article{santambrogio2015optimal,
  title={Optimal transport for applied mathematicians},
  author={Santambrogio, Filippo},
  journal={Birk{\"a}user, NY},
  volume={55},
  number={58-63},
  pages={94},
  year={2015},
  publisher={Springer}
}
@book{villani2008optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2008},
  publisher={Springer Science \& Business Media}
}
@inproceedings{Dabney2018DistributionalRL,
  title={Distributional Reinforcement Learning with Quantile Regression},
  author={W. Dabney and M. Rowland and Marc G. Bellemare and R. Munos},
  booktitle={AAAI},
  year={2018}
}
@inproceedings{Bellemare2017ADP,
  title={A Distributional Perspective on Reinforcement Learning},
  author={Marc G. Bellemare and W. Dabney and R. Munos},
  booktitle={ICML},
  year={2017}
}
@article{JMLR:v21:19-144,
  author  = {Haoran Wang and Thaleia Zariphopoulou and Xun Yu Zhou},
  title   = {Reinforcement Learning in Continuous Time and Space: A Stochastic Control Approach},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {198},
  pages   = {1-34},
  url     = {http://jmlr.org/papers/v21/19-144.html}
}
@article{bellemare2017cramer,
  title={The cramer distance as a solution to biased wasserstein gradients},
  author={Bellemare, Marc G and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1705.10743},
  year={2017}
}
@article{chow2019algorithms,
author = {Chow, Yat Tin and Li, Wuchen and Osher, Stanley and Yin, Wotao},
title = {Algorithm for Hamilton---Jacobi Equations in Density Space Via a Generalized Hopf Formula},
year = {2019},
publisher = {Plenum Press},
address = {USA},
volume = {80},
number = {2},
issn = {0885-7474},
url = {https://doi.org/10.1007/s10915-019-00972-9},
doi = {10.1007/s10915-019-00972-9},
abstract = {We design fast numerical methods for Hamilton---Jacobi equations in density space (HJD), which arises in optimal transport and mean field games. We proposes an algorithm using a generalized Hopf formula in density space. The formula helps transforming a problem from an optimal control problem in density space, which are constrained minimizations supported on both spatial and time variables, to an optimization problem over only one spatial variable. This transformation allows us to compute HJD efficiently via multi-level approaches and coordinate descent methods. Rigorous derivation of the Hopf formula is provided under restricted assumptions and for a relatively narrow case; meanwhile our practical investigation allows us to conjecture that the actual range of applicability should be wider, and therefore we conjecture the formula can be applied to a wider class of practical examples.},
journal = {J. Sci. Comput.},
month = aug,
pages = {1195–1239},
numpages = {45},
keywords = {Generalized Hopf formula, Optimal transport, Hamilton---Jacobi equation in density space, Mean field games}
}
@article{miersemann2012calculus,
  title={Calculus of variations},
  author={Miersemann, Erich},
  journal={Lecture notes, Department of Mathematics, Leipzig University},
  year={2012}
}
@article{bertsekas2003convex,
  title={Convex analysis and optimization, ser},
  author={Bertsekas, DP and Nedic, A and Ozdaglar, A},
  journal={Athena Scientific optimization and computation series. Athena Scientific},
  year={2003}
}
@article{Santambrogio2016EuclideanMA,
  title={\{Euclidean, metric, and Wasserstein\} gradient flows: an overview},
  author={F. Santambrogio},
  journal={Bulletin of Mathematical Sciences},
  year={2016},
  volume={7},
  pages={87-154}
}
@book{ullrich2011time,
  title={Time-dependent density-functional theory: concepts and applications},
  author={Ullrich, Carsten A},
  year={2011},
  publisher={OUP Oxford}
}
@inproceedings{Zhang2018PolicyOA,
  title={Policy Optimization as Wasserstein Gradient Flows},
  author={Ruiyi Zhang and C. Chen and C. Li and L. Carin},
  booktitle={ICML},
  year={2018}
}
@article{Duncan1971OnTS,
  title={On the Solutions of a Stochastic Control System. II},
  author={T. Duncan and P. Varaiya},
  journal={Siam Journal on Control},
  year={1971},
  volume={13},
  pages={1077-1092}
}
@book{le2016brownian,
  title={Brownian motion, martingales, and stochastic calculus},
  author={Le Gall, Jean-Fran{\c{c}}ois},
  year={2016},
  publisher={Springer}
}
@book{kreyszig1978introductory,
  title={Introductory functional analysis with applications},
  author={Kreyszig, Erwin},
  volume={1},
  year={1978},
  publisher={wiley New York}
}
@inproceedings{lecun1993automatic,
  title={Automatic learning rate maximization by on-line estimation of the hessian's eigenvectors},
  author={LeCun, Yann and Simard, Patrice Y and Pearlmutter, Barak A},
  year={1993},
  organization={Neural Information Processing Systems (NIPS)}
}
@article{rogers1994diffusions,
  title={Diffusions, Markov Processes and Martingales, Volume 1: Foundations},
  author={Rogers, L Chris G and Williams, David},
  journal={John Wiley \& Sons, Ltd., Chichester},
  volume={7},
  year={1994}
}
@misc{thorpeintroduction,
  title={Introduction to Optimal Transport},
  author={Thorpe, Matthew},
  url={https://www.math.cmu.edu/~mthorpe/OTNotes.pdf},
  year={2018}
}
@ARTICLE{Jordan02thevariational,
    author = {Richard Jordan and David Kinderlehrer and Felix Otto},
    title = {The Variational Formulation of the Fokker-Planck Equation},
    journal = {SIAM J. Math. Anal},
    year = {2002},
    volume = {29},
    pages = {1--17}
}
@misc{liu2019stein,
      title={Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm}, 
      author={Qiang Liu and Dilin Wang},
      year={2019},
      eprint={1608.04471},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{JMLR:v22:20-168,
  author  = {Umit Köse and Andrzej Ruszczyński},
  title   = {Risk-Averse Learning by Temporal Difference Methods with Markov Risk Measures},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {38},
  pages   = {1-34},
  url     = {http://jmlr.org/papers/v22/20-168.html}
}
@inproceedings{salim2020wasserstein,
 author = {Salim, Adil and Korba, Anna and Luise, Giulia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12356--12366},
 publisher = {Curran Associates, Inc.},
 title = {The Wasserstein Proximal Gradient Algorithm},
 url = {https://proceedings.neurips.cc/paper/2020/file/91cff01af640a24e7f9f7a5ab407889f-Paper.pdf},
 volume = {33},
 year = {2020}
}
@article{BOWLES201530,
title = {Weak solutions to a fractional Fokker–Planck equation via splitting and Wasserstein gradient flow},
journal = {Applied Mathematics Letters},
volume = {42},
pages = {30-35},
year = {2015},
issn = {0893-9659},
doi = {https://doi.org/10.1016/j.aml.2014.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893965914003346},
author = {Malcolm Bowles and Martial Agueh},
keywords = {Fractional Laplacian, Splitting, Wasserstein gradient flow},
abstract = {We study a linear fractional Fokker–Planck equation that models non-local diffusion in the presence of a potential field. The non-locality is due to the appearance of the ‘fractional Laplacian’ in the corresponding PDE, in place of the classical Laplacian which distinguishes the case of regular diffusion. We prove existence of weak solutions by combining a splitting technique together with a Wasserstein gradient flow formulation. An explicit iterative construction is given, which we prove weakly converges to a weak solution of this PDE.}
}
@book{fleming2006controlled,
  title={Controlled Markov processes and viscosity solutions},
  author={Fleming, Wendell H and Soner, Halil Mete},
  volume={25},
  year={2006},
  publisher={Springer Science \& Business Media}
}
@book{ambrosio2008gradient,
  title={Gradient flows: in metric spaces and in the space of probability measures},
  author={Ambrosio, Luigi and Gigli, Nicola and Savar{\'e}, Giuseppe},
  year={2008},
  publisher={Springer Science \& Business Media}
}
@book{kim2013functional,
  title={Functional Differential Equations: Application of i-smooth calculus},
  author={Kim, AV},
  volume={479},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@book{harrison2013brownian,
  title={Brownian models of performance and control},
  author={Harrison, J Michael},
  year={2013},
  publisher={Cambridge University Press}
}
@article{crandall1983viscosity,
  title={Viscosity solutions of Hamilton-Jacobi equations},
  author={Crandall, Michael G and Lions, Pierre-Louis},
  journal={Transactions of the American mathematical society},
  volume={277},
  number={1},
  pages={1--42},
  year={1983}
}
@book{duriez2017machine,
  title={Machine learning control-taming nonlinear dynamics and turbulence},
  author={Duriez, Thomas and Brunton, Steven L and Noack, Bernd R},
  year={2017},
  publisher={Springer}
}
@article{Wang2019,
   title={Learning Deep Stochastic Optimal Control Policies Using Forward-Backward SDEs},
   ISBN={9780992374754},
   url={http://dx.doi.org/10.15607/RSS.2019.XV.070},
   DOI={10.15607/rss.2019.xv.070},
   journal={Robotics: Science and Systems XV},
   publisher={Robotics: Science and Systems Foundation},
   author={Wang, Ziyi and Pereira, Marcus and Ioannis Exarchos and Theodorou, Evangelos},
   year={2019},
   month={Jun}
}
@article{SILVER2021103535,
title = {Reward Is Enough},
journal = {Artificial Intelligence},
pages = {103535},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103535},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
author = {David Silver and Satinder Singh and Doina Precup and Richard S. Sutton},
keywords = {Artificial Intelligence, Artificial general intelligence, Reinforcement learning, reward},
abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.}
}
@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}
@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{badia2020agent57,
  title={Agent57: Outperforming the atari human benchmark},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Zhaohan Daniel and Blundell, Charles},
  booktitle={International Conference on Machine Learning},
  pages={507--517},
  year={2020},
  organization={PMLR}
}
@inproceedings{higuera2018synthesizing,
  title={Synthesizing neural network controllers with probabilistic model-based reinforcement learning},
  author={Higuera, Juan Camilo Gamboa and Meger, David and Dudek, Gregory},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={2538--2544},
  year={2018},
  organization={IEEE}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@techreport{bellman1954theory,
  title={The theory of dynamic programming},
  author={Bellman, Richard},
  year={1954},
  institution={Rand corp santa monica ca}
}
@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}
@book{yoshida1957lectures,
  title={Lectures on semi-group theory and its application to Cauchy's problem in partial differential equations},
  author={Yoshida, K{\=o}saku and Narasimhan, Madumbai S},
  volume={8},
  year={1957},
  publisher={Tata Institute of Fundamental Research Bombay, India}
}
@article{ohnishi2018continuous,
  author       = {Motoya Ohnishi AND Masahiro Yukawa AND Mikael Johansson AND Masashi Sugiyama},
  title	       = {{Continuous-time Value Function Approximation in Reproducing Kernel Hilbert Spaces}},
  year	       = {2018},
  archiveprefix= {arXiv},
  eprint       = {1806.02985v3},
  primaryclass = {math.OC}
}
@article{theodorou2010generalized,
  title={A generalized path integral control approach to reinforcement learning},
  author={Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={3137--3181},
  year={2010},
  publisher={JMLR. org}
}
@misc{li2021semigroup,
      title={A semigroup method for high dimensional elliptic PDEs and eigenvalue problems based on neural networks}, 
      author={Haoya Li and Lexing Ying},
      year={2021},
      eprint={2105.03480},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}
@misc{li2021semigroupcommitor,
      title={A semigroup method for high dimensional committor functions based on neural network}, 
      author={Haoya Li and Yuehaw Khoo and Yinuo Ren and Lexing Ying},
      year={2021},
      eprint={2012.06727},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}
@book{cohn2013measure,
  title={Measure theory},
  author={Cohn, Donald L},
  year={2013},
  publisher={Springer}
}
@InProceedings{pmlr-v139-alvarez-melis21a,
  title = 	 {Dataset Dynamics via Gradient Flows in Probability Space},
  author =       {Alvarez-Melis, David and Fusi, Nicol\`o},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {219--230},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/alvarez-melis21a/alvarez-melis21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/alvarez-melis21a.html},
  abstract = 	 {Various machine learning tasks, from generative modeling to domain adaptation, revolve around the concept of dataset transformation and manipulation. While various methods exist for transforming unlabeled datasets, principled methods to do so for labeled (e.g., classification) datasets are missing. In this work, we propose a novel framework for dataset transformation, which we cast as optimization over data-generating joint probability distributions. We approach this class of problems through Wasserstein gradient flows in probability space, and derive practical and efficient particle-based methods for a flexible but well-behaved class of objective functions. Through various experiments, we show that this framework can be used to impose constraints on classification datasets, adapt them for transfer learning, or to re-purpose fixed or black-box models to classify {—}with high accuracy{—} previously unseen datasets.}
}
@InProceedings{pmlr-v139-yildiz21a,
  title = 	 {Continuous-time Model-based Reinforcement Learning},
  author =       {Yildiz, Cagatay and Heinonen, Markus and L{\"a}hdesm{\"a}ki, Harri},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12009--12018},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yildiz21a/yildiz21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/yildiz21a.html},
  abstract = 	 {Model-based reinforcement learning (MBRL) approaches rely on discrete-time state transition models whereas physical systems and the vast majority of control tasks operate in continuous-time. To avoid time-discretization approximation of the underlying process, we propose a continuous-time MBRL framework based on a novel actor-critic method. Our approach also infers the unknown state evolution differentials with Bayesian neural ordinary differential equations (ODE) to account for epistemic uncertainty. We implement and test our method on a new ODE-RL suite that explicitly solves continuous-time control systems. Our experiments illustrate that the model is robust against irregular and noisy data, and can solve classic control problems in a sample-efficient manner.}
}
@inproceedings{rasmussen1999infinite,
  title={The infinite Gaussian mixture model.},
  author={Rasmussen, Carl Edward and others},
  booktitle={NIPS},
  volume={12},
  pages={554--560},
  year={1999}
}
@article{chen2014self,
  title={A self-adaptive Gaussian mixture model},
  author={Chen, Zezhi and Ellis, Tim},
  journal={Computer Vision and Image Understanding},
  volume={122},
  pages={35--46},
  year={2014},
  publisher={Elsevier}
}
@article{geman1984stochastic,
  title={Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images},
  author={Geman, Stuart and Geman, Donald},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={6},
  pages={721--741},
  year={1984},
  publisher={IEEE}
}
@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher K and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT press Cambridge, MA}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{bishop2006pattern,
  title={Pattern recognition},
  author={Bishop, Christopher M},
  journal={Machine learning},
  volume={128},
  number={9},
  year={2006}
}
@book{aluffi2009algebra,
  title={Algebra: chapter 0},
  author={Aluffi, Paolo},
  volume={104},
  year={2009},
  publisher={American Mathematical Soc.}
}
@article{bellemare19distr,
  author       = {Marc G. Bellemare AND Nicolas Le Roux AND Pablo
                  Samuel Castro AND Subhodeep Moitra},
  title	       = {{Distributional reinforcement learning with linear
                  function approximation}},
  year	       = 2019,
  archiveprefix= {arXiv},
  eprint       = {1902.03149v1},
  primaryclass = {cs.LG}
}
@inproceedings{NEURIPS2019_f471223d,
 author = {Yang, Derek and Zhao, Li and Lin, Zichuan and Qin, Tao and Bian, Jiang and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fully Parameterized Quantile Function for Distributional Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/f471223d1a1614b58a7dc45c9d01df19-Paper.pdf},
 volume = {32},
 year = {2019}
}
@article{nguyen2020distributional,
  author       = {Thanh Tang Nguyen AND Sunil Gupta AND Svetha
                  Venkatesh},
  title	       = {{Distributional Reinforcement Learning via Moment
                  Matching}},
  year	       = 2020,
  archiveprefix= {arXiv},
  eprint       = {2007.12354v3},
  primaryclass = {cs.LG}
}
@article{martin2019stochastically,
  author       = {John D. Martin AND Michal Lyskawinski AND Xiaohu Li
                  AND Brendan Englot},
  title	       = {{Stochastically Dominant Distributional
                  Reinforcement Learning}},
  year	       = 2019,
  archiveprefix= {arXiv},
  eprint       = {1905.07318v4},
  primaryclass = {cs.LG}
}
@book{tao2010epsilon,
  title={An Epsilon of Room, II: pages from year three of a mathematical blog},
  author={Tao, Terence},
  year={2010},
  publisher={American Mathematical Society Providence, RI}
}
@article{lewin1991simple,
  title={A simple proof of Zorn's lemma},
  author={Lewin, Jonathan W},
  journal={The American mathematical monthly},
  volume={98},
  number={4},
  pages={353},
  year={1991}
}
@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}
@inproceedings{wibisono2018sampling,
  title={Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem},
  author={Wibisono, Andre},
  booktitle={Conference on Learning Theory},
  pages={2093--3027},
  year={2018},
  organization={PMLR}
}
@inproceedings{raginsky2017non,
  title={Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1674--1703},
  year={2017},
  organization={PMLR}
}
@misc{martin2020stochastically,
      title={Stochastically Dominant Distributional Reinforcement Learning}, 
      author={John D. Martin and Michal Lyskawinski and Xiaohu Li and Brendan Englot},
      year={2020},
      eprint={1905.07318},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{girsanov1960transforming,
  title={On transforming a certain class of stochastic processes by absolutely continuous substitution of measures},
  author={Girsanov, Igor Vladimirovich},
  journal={Theory of Probability \& Its Applications},
  volume={5},
  number={3},
  pages={285--301},
  year={1960},
  publisher={SIAM}
}
@inproceedings{dabney2018implicit,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={1096--1105},
  year={2018},
  organization={PMLR}
}
@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-second AAAI conference on artificial intelligence},
  year={2018}
}
@book{oliehoek2016concise,
  title={A concise introduction to decentralized POMDPs},
  author={Oliehoek, Frans A and Amato, Christopher},
  year={2016},
  publisher={Springer}
}
@inproceedings{hu1998multiagent,
  title={Multiagent reinforcement learning: theoretical framework and an algorithm.},
  author={Hu, Junling and Wellman, Michael P and others},
  booktitle={ICML},
  volume={98},
  pages={242--250},
  year={1998},
  organization={Citeseer}
}
@article{bellemare2020autonomous,
  title={Autonomous navigation of stratospheric balloons using reinforcement learning},
  author={Bellemare, Marc G and Candido, Salvatore and Castro, Pablo Samuel and Gong, Jun and Machado, Marlos C and Moitra, Subhodeep and Ponda, Sameera S and Wang, Ziyu},
  journal={Nature},
  volume={588},
  number={7836},
  pages={77--82},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{hafner2019dream,
  title={Dream to control: Learning behaviors by latent imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2019}
}
@book{fleming2012deterministic,
  title={Deterministic and stochastic optimal control},
  author={Fleming, Wendell H and Rishel, Raymond W},
  volume={1},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@article{sun2021dfac,
  title={DFAC framework: Factorizing the value function via quantile mixture for multi-agent distributional q-learning},
  author={Sun, Wei-Fang and Lee, Cheng-Kuang and Lee, Chun-Yi},
  journal={arXiv preprint arXiv:2102.07936},
  year={2021}
}
@inproceedings{hostallero2019learning,
  title={Learning to factorize with transformation for cooperative multi-agent reinforcement learning},
  author={Hostallero, Wan Ju Kang David Earl and Son, Kyunghwan and Kim, Daewoo and Qtran, Yung Yi},
  booktitle={Proceedings of the 31st International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR},
  year={2019}
}
@InProceedings{florianICRA2018,
  author = {Shkurti, Florian and Kakodkar, Nikhil and Dudek, Gregory},
  title = {{Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning}},
  booktitle = "IEEE International Conference on Robotics and Automation (ICRA)",
  year = "2018",
  pages = "7804-7811",
  month = "May",
  address = "Brisbane, Australia"
}
@InProceedings{pmlr-v80-fujimoto18a,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author = 	 {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1587--1596},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract = 	 {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}
@inproceedings{gelada2019deepmdp,
  title={Deepmdp: Learning continuous latent space models for representation learning},
  author={Gelada, Carles and Kumar, Saurabh and Buckman, Jacob and Nachum, Ofir and Bellemare, Marc G},
  booktitle={International Conference on Machine Learning},
  pages={2170--2179},
  year={2019},
  organization={PMLR}
}
@article{bellemare2019geometric,
  title={A geometric perspective on optimal representations for reinforcement learning},
  author={Bellemare, Marc and Dabney, Will and Dadashi, Robert and Ali Taiga, Adrien and Castro, Pablo Samuel and Le Roux, Nicolas and Schuurmans, Dale and Lattimore, Tor and Lyle, Clare},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={4358--4369},
  year={2019}
}
@article{taiga2021bonus,
  title={On Bonus-Based Exploration Methods in the Arcade Learning Environment},
  author={Taiga, Adrien Ali and Fedus, William and Machado, Marlos C and Courville, Aaron and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2109.11052},
  year={2021}
}
@article{bertini1995stochastic,
  title={The stochastic heat equation: Feynman-Kac formula and intermittence},
  author={Bertini, Lorenzo and Cancrini, Nicoletta},
  journal={Journal of statistical Physics},
  volume={78},
  number={5},
  pages={1377--1401},
  year={1995},
  publisher={Springer}
}
@article{hastings1970monte,
  title={Monte Carlo sampling methods using Markov chains and their applications},
  author={Hastings, W Keith},
  year={1970},
  publisher={Oxford University Press}
}
@article{roberts2002langevin,
  title={Langevin diffusions and Metropolis-Hastings algorithms},
  author={Roberts, Gareth O and Stramer, Osnat},
  journal={Methodology and computing in applied probability},
  volume={4},
  number={4},
  pages={337--357},
  year={2002},
  publisher={Springer}
}
@InProceedings{pmlr-v70-haarnoja17a,
  title = 	 {Reinforcement Learning with Deep Energy-Based Policies},
  author =       {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1352--1361},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/haarnoja17a.html},
  abstract = 	 {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.}
}
@article{koenker2001quantile,
  title={Quantile regression},
  author={Koenker, Roger and Hallock, Kevin F},
  journal={Journal of economic perspectives},
  volume={15},
  number={4},
  pages={143--156},
  year={2001}
}
@article{koenker1978regression,
  title={Regression quantiles},
  author={Koenker, Roger and Bassett Jr, Gilbert},
  journal={Econometrica: journal of the Econometric Society},
  pages={33--50},
  year={1978},
  publisher={JSTOR}
}
@misc{cuturi2013sinkhorn,
      title={Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances}, 
      author={Marco Cuturi},
      year={2013},
      eprint={1306.0895},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{gronwall1919note,
  title={Note on the derivatives with respect to a parameter of the solutions of a system of differential equations},
  author={Gronwall, Thomas Hakon},
  journal={Annals of Mathematics},
  pages={292--296},
  year={1919},
  publisher={JSTOR}
}
@book{rockafellar2015convex,
  title={Convex analysis},
  author={Rockafellar, Ralph Tyrell},
  year={2015},
  publisher={Princeton university press}
}
@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1805.09545},
  year={2018}
}
@article{de1993new,
  title={New problems on minimizing movements},
  author={De Giorgi, Ennio},
  journal={Ennio de Giorgi: Selected Papers},
  pages={699--713},
  year={1993}
}
@article{sinkhorn1967diagonal,
  title={Diagonal equivalence to matrices with prescribed row and column sums},
  author={Sinkhorn, Richard},
  journal={The American Mathematical Monthly},
  volume={74},
  number={4},
  pages={402--405},
  year={1967},
  publisher={JSTOR}
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}
@article{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  publisher={King's College, Cambridge United Kingdom}
}
@book{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  year={1996},
  publisher={Athena Scientific}
}
@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992},
  publisher={Springer}
}
@book{friedman2017elements,
  title={The elements of statistical learning: Data mining, inference, and prediction},
  author={Friedman, Jerome H},
  year={2017},
  publisher={springer open}
}
@inproceedings{munos1997convergent,
  title={A convergent reinforcement learning algorithm in the continuous case based on a finite difference method},
  author={Munos, R{\'e}mi},
  booktitle={IJCAI (2)},
  pages={826--831},
  year={1997}
}
@phdthesis{sutton1984temporal,
  title={Temporal credit assignment in reinforcement learning},
  author={Sutton, Richard Stuart},
  year={1984},
  school={University of Massachusetts Amherst}
}
@article{arumugam2021information,
  title={An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning},
  author={Arumugam, Dilip and Henderson, Peter and Bacon, Pierre-Luc},
  journal={arXiv preprint arXiv:2103.06224},
  year={2021}
}
@techreport{baird1993advantage,
  title={Advantage updating},
  author={Baird III, Leemon C},
  year={1993},
  institution={WRIGHT LAB WRIGHT-PATTERSON AFB OH}
}
@inproceedings{baird1994reinforcement,
  title={Reinforcement learning in continuous time: Advantage updating},
  author={Baird, Leemon C},
  booktitle={Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)},
  volume={4},
  pages={2448--2453},
  year={1994},
  organization={IEEE}
}
@article{kim2021hamilton,
  title={Hamilton-jacobi deep q-learning for deterministic continuous-time systems with lipschitz continuous controls},
  author={Kim, Jeongho and Shin, Jaeuk and Yang, Insoon},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={206},
  pages={1--34},
  year={2021}
}
@article{doya2000reinforcement,
  title={Reinforcement learning in continuous time and space},
  author={Doya, Kenji},
  journal={Neural computation},
  volume={12},
  number={1},
  pages={219--245},
  year={2000},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@inproceedings{munos1997reinforcement,
  title={Reinforcement learning for continuous stochastic control problems},
  author={Munos, R{\'e}mi and Bourgine, Paul},
  booktitle={NIPS},
  pages={1029--1035},
  year={1997}
}
@article{pereira2019learning,
  title={Learning deep stochastic optimal control policies using forward-backward sdes},
  author={Pereira, Marcus and Wang, Ziyi and Exarchos, Ioannis and Theodorou, Evangelos A},
  journal={arXiv preprint arXiv:1902.03986},
  year={2019}
}
@article{exarchos2018stochastic,
  title={Stochastic optimal control via forward and backward stochastic differential equations and importance sampling},
  author={Exarchos, Ioannis and Theodorou, Evangelos A},
  journal={Automatica},
  volume={87},
  pages={159--165},
  year={2018},
  publisher={Elsevier}
}
@article{tassa2007least,
  title={Least squares solutions of the HJB equation with neural network value-function approximators},
  author={Tassa, Yuval and Erez, Tom},
  journal={IEEE transactions on neural networks},
  volume={18},
  number={4},
  pages={1031--1041},
  year={2007},
  publisher={IEEE}
}
@article{lutter2021value,
  title={Value Iteration in Continuous Actions, States and Time},
  author={Lutter, Michael and Mannor, Shie and Peters, Jan and Fox, Dieter and Garg, Animesh},
  journal={arXiv preprint arXiv:2105.04682},
  year={2021}
}
@article{lutter2021continuous,
  title={Continuous-Time Fitted Value Iteration for Robust Policies},
  author={Lutter, Michael and Belousov, Boris and Mannor, Shie and Fox, Dieter and Garg, Animesh and Peters, Jan},
  journal={arXiv preprint arXiv:2110.01954},
  year={2021}
}
@inproceedings{tallec2019making,
  title={Making deep q-learning methods robust to time discretization},
  author={Tallec, Corentin and Blier, L{\'e}onard and Ollivier, Yann},
  booktitle={International Conference on Machine Learning},
  pages={6096--6104},
  year={2019},
  organization={PMLR}
}
@article{rosler1992fixed,
  title={A fixed point theorem for distributions},
  author={R{\"o}sler, Uwe},
  journal={Stochastic Processes and their Applications},
  volume={42},
  number={2},
  pages={195--214},
  year={1992},
  publisher={Elsevier}
}
@inproceedings{mavrin2019distributional,
  title={Distributional reinforcement learning for efficient exploration},
  author={Mavrin, Borislav and Yao, Hengshuai and Kong, Linglong and Wu, Kaiwen and Yu, Yaoliang},
  booktitle={International conference on machine learning},
  pages={4424--4434},
  year={2019},
  organization={PMLR}
}
@article{zhang2021safe,
  title={Safe Distributional Reinforcement Learning},
  author={Zhang, Jianyi and Weng, Paul},
  journal={arXiv preprint arXiv:2102.13446},
  year={2021}
}
@article{halperin2021distributional,
  title={Distributional Offline Continuous-Time Reinforcement Learning with Neural Physics-Informed PDEs (SciPhy RL for DOCTR-L)},
  author={Halperin, Igor},
  journal={arXiv preprint arXiv:2104.01040},
  year={2021}
}
@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}
@inproceedings{henderson2018deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}
@article{barto1981associative,
  title={Associative search network: A reinforcement learning associative memory},
  author={Barto, Andrew G and Sutton, Richard S and Brouwer, Peter S},
  journal={Biological cybernetics},
  volume={40},
  number={3},
  pages={201--211},
  year={1981},
  publisher={Springer}
}
@article{tesauro1994td,
  title={TD-Gammon, a self-teaching backgammon program, achieves master-level play},
  author={Tesauro, Gerald},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={215--219},
  year={1994},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@inproceedings{lin1993scaling,
  title={Scaling up reinforcement learning for robot control},
  author={Lin, Long Ji},
  booktitle={ICML},
  year={1993}
}
@inproceedings{smart2002effective,
  title={Effective reinforcement learning for mobile robots},
  author={Smart, William D and Kaelbling, L Pack},
  booktitle={Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292)},
  volume={4},
  pages={3404--3410},
  year={2002},
  organization={IEEE}
}
@inproceedings{peters2003reinforcement,
  title={Reinforcement learning for humanoid robotics},
  author={Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
  booktitle={Proceedings of the third IEEE-RAS international conference on humanoid robots},
  pages={1--20},
  year={2003}
}
@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
@article{bellman1957markovian,
  title={A Markovian decision process},
  author={Bellman, Richard},
  journal={Journal of mathematics and mechanics},
  volume={6},
  number={5},
  pages={679--684},
  year={1957},
  publisher={JSTOR}
}
@incollection{hoeffding1994probability,
  title={Probability inequalities for sums of bounded random variables},
  author={Hoeffding, Wassily},
  booktitle={The collected works of Wassily Hoeffding},
  pages={409--426},
  year={1994},
  publisher={Springer}
}
@article{hoeffdingbound,
author = { Wassily   Hoeffding },
title = {Probability Inequalities for Sums of Bounded Random Variables},
journal = {Journal of the American Statistical Association},
volume = {58},
number = {301},
pages = {13-30},
year  = {1963},
publisher = {Taylor \& Francis},
doi = {10.1080/01621459.1963.10500830},
URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500830
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1963.10500830
}
}
@article{banach1924decomposition,
  title={Sur la d{\'e}composition des ensembles de points en parties respectivement congruentes},
  author={Banach, Stefan and Tarski, Alfred},
  journal={Fund. math},
  volume={6},
  number={1},
  pages={244--277},
  year={1924}
}
@inproceedings{bellemare2016increasing,
  title={Increasing the action gap: New operators for reinforcement learning},
  author={Bellemare, Marc G and Ostrovski, Georg and Guez, Arthur and Thomas, Philip and Munos, R{\'e}mi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}
@article{howard1960dynamic,
  title={Dynamic programming and markov processes.},
  author={Howard, Ronald A},
  year={1960},
  publisher={John Wiley}
}
@article{puterman1979convergence,
  title={On the convergence of policy iteration in stationary dynamic programming},
  author={Puterman, Martin L and Brumelle, Shelby L},
  journal={Mathematics of Operations Research},
  volume={4},
  number={1},
  pages={60--69},
  year={1979},
  publisher={INFORMS}
}
@article{agarwal2019reinforcement,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  year={2019}
}
@article{kac1949distributions,
  title={On distributions of certain Wiener functionals},
  author={Kac, Mark},
  journal={Transactions of the American Mathematical Society},
  volume={65},
  number={1},
  pages={1--13},
  year={1949}
}
@book{lax2002functional,
  title={Functional Analysis},
  author={Lax, P.D. and John Wiley \& Sons},
  isbn={9780471556046},
  lccn={01046547},
  series={Pure and Applied Mathematics: A Wiley Series of Texts, Monographs and Tracts},
  url={https://books.google.ca/books?id=-jbvAAAAMAAJ},
  year={2002},
  publisher={Wiley}
}
@article{kolmogoroff1931analytischen,
  title={{\"U}ber die analytischen Methoden in der Wahrscheinlichkeitsrechnung},
  author={Kolmogoroff, Andrei},
  journal={Mathematische Annalen},
  volume={104},
  number={1},
  pages={415--458},
  year={1931},
  publisher={Springer}
}
@article{de1980problems,
  title={Problems of evolution in metric spaces and maximal decreasing curve},
  author={De Giorgi, Ennio and Marino, Antonio and Tosques, Mario},
  journal={Atti Accad. Naz. Lincei Rend. Cl. Sci. Fis. Mat. Natur.(8)},
  volume={68},
  number={3},
  pages={180--187},
  year={1980}
}
@misc{muratori2018gradient,
      title={Gradient flows and Evolution Variational Inequalities in metric spaces. I: structural properties}, 
      author={Matteo Muratori and Giuseppe Savaré},
      year={2018},
      eprint={1810.03939},
      archivePrefix={arXiv},
      primaryClass={math.FA}
}
@article{benamou2000computational,
  title={A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem},
  author={Benamou, Jean-David and Brenier, Yann},
  journal={Numerische Mathematik},
  volume={84},
  number={3},
  pages={375--393},
  year={2000},
  publisher={Springer}
}
@misc{heyde1970convergence,
  title={CONVERGENCE OF PROBABILITY MEASURES-BILLINGSLEY, P},
  author={HEYDE, CC},
  year={1970},
  publisher={AUSTRALIAN STATISTICAL PUBLISHING ASSOC INC C/O MR E BRINGLEY, TREASURER~…}
}
@book{bdr2022,
    title={Distributional Reinforcement Learning},
    author={Marc G. Bellemare and Will Dabney and Mark Rowland},
    publisher={MIT Press},
    note={\url{http://www.distributional-rl.org}},
    year={2022}
}
@inproceedings{amortila2020distributional,
  title={A distributional analysis of sampling-based reinforcement learning algorithms},
  author={Amortila, Philip and Precup, Doina and Panangaden, Prakash and Bellemare, Marc G},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4357--4366},
  year={2020},
  organization={PMLR}
}
@article{brouwer1911abbildung,
  title={{\"U}ber abbildung von mannigfaltigkeiten},
  author={Brouwer, Luitzen Egbertus Jan},
  journal={Mathematische annalen},
  volume={71},
  number={1},
  pages={97--115},
  year={1911},
  publisher={Springer}
}
@article{schauder1930fixpunktsatz,
  title={Der Fixpunktsatz in Funktional-raumen},
  author={Schauder, Juliusz},
  journal={Studia math.},
  volume={2},
  pages={171--180},
  year={1930}
}
@article{kellogg1976uniqueness,
  title={Uniqueness in the Schauder fixed point theorem},
  author={Kellogg, RB},
  journal={Proceedings of the American Mathematical Society},
  volume={60},
  number={1},
  pages={207--210},
  year={1976}
}
@article{teschl2021topics,
  title={Topics in Functional Analysis},
  author={Teschl, Gerald},
  year={2021}
}
@article{hale1964functional,
  title={Functional Analysis: Topological Methods in the Theory of Nonlinear Integral Equations. MA Krasnosel'skii. Translated from the Russian edition (Moscow, 1956) by AH Armstrong. J. Burlak, Ed. Pergamon, London; Macmillan, New York, 1964. xii+ 395 pp. Illus. \$10.},
  author={Hale, JK},
  journal={Science},
  volume={145},
  number={3633},
  pages={697--697},
  year={1964},
  publisher={American Association for the Advancement of Science}
}
@article{zhang21made,
  author       = {Tianjun Zhang AND Paria Rashidinejad AND Jiantao Jiao AND Yuandong Tian AND Joseph Gonzalez AND Stuart Russell},
  title	       = {{MADE: Exploration via Maximizing Deviation from Explored Regions}},
  year	       = 2021,
  archiveprefix= {arXiv},
  eprint       = {2106.10268v1},
  primaryclass = {cs.LG}
}
@article{solin21scalable,
  author       = {Arno Solin AND Ella Tamir AND Prakhar Verma},
  title	       = {{Scalable Inference in SDEs by Direct Matching of the Fokker-Planck-Kolmogorov Equation}},
  year	       = 2021,
  archiveprefix= {arXiv},
  eprint       = {2110.15739v1},
  primaryclass = {cs.LG}
}
@article{allen21learn,
  author       = {Cameron Allen AND Neev Parikh AND Omer Gottesman AND George Konidaris},
  title	       = {{Learning Markov State Abstractions for Deep Reinforcement Learning}},
  year	       = 2021,
  archiveprefix= {arXiv},
  eprint       = {2106.04379v3},
  primaryclass = {cs.LG}
}
@article{wang2021deep,
  author = {Gefei Wang AND Yuling Jiao AND Qian Xu AND Yang Wang AND Can Yang},
  title = {{Deep Generative Learning via Schrödinger Bridge}},
  year = {2021},
  archivePrefix = {arXiv},
  eprint = {2106.10410v2},
  primaryClass = {cs.LG}
}
@article{chen2021likelihood,
  author = {Tianrong Chen AND Guan-Horng Liu AND Evangelos A. Theodorou},
  title = {{Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs
    Theory}},
  year = {2021},
  archivePrefix = {arXiv},
  eprint = {2110.11291v2},
  primaryClass = {stat.ML}
}
@article{schubert2021learning,
author = {Ingmar Schubert AND Danny Driess AND Ozgur S. Oguz AND Marc Toussaint},
title = {{Learning to Execute: Efficient Learning of Universal Plan-Conditioned
  Policies in Robotics}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2111.07908v1},
primaryClass = {cs.AI}}
@article{pan2021regularized,
author = {Ling Pan AND Tabish Rashid AND Bei Peng AND Longbo Huang AND Shimon Whiteson},
title = {{Regularized Softmax Deep Multi-Agent $Q$-Learning}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2103.11883v2},
primaryClass = {cs.LG}}
@article{rigter2021risk-averse,
author = {Marc Rigter AND Bruno Lacerda AND Nick Hawes},
title = {{Risk-Averse Bayes-Adaptive Reinforcement Learning}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2102.05762v2},
primaryClass = {cs.LG}}
@inproceedings{bather1967sequential,
  title={Sequential decisions in the control of a spaceship},
  author={Bather, JA and Chernoff, Herman},
  booktitle={Fifth Berkeley Symposium on Mathematical Statistics and Probability},
  volume={3},
  pages={181--207},
  year={1967}
}
@inproceedings{Krylov1980ControlledDP,
  title={Controlled Diffusion Processes},
  author={Nikolai Vladimirovich Krylov},
  year={1980}
}
@article{eysenbach2021robust,
author = {Benjamin Eysenbach AND Ruslan Salakhutdinov AND Sergey Levine},
title = {{Robust Predictable Control}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2109.03214v1},
primaryClass = {cs.LG}}
@article{park2021time,
author = {Seohong Park AND Jaekyeom Kim AND Gunhee Kim},
title = {{Time Discretization-Invariant Safe Action Repetition for Policy Gradient
  Methods}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2111.03941v4},
primaryClass = {cs.LG}}
@article{leonardos2021exploration-exploitation,
author = {Stefanos Leonardos AND Georgios Piliouras AND Kelly Spendlove},
title = {{Exploration-Exploitation in Multi-Agent Competition: Convergence with
  Bounded Rationality}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2106.12928v1},
primaryClass = {cs.GT}}
@article{bauerle2011markov,
  title={Markov decision processes with average-value-at-risk criteria},
  author={B{\"a}uerle, Nicole and Ott, Jonathan},
  journal={Mathematical Methods of Operations Research},
  volume={74},
  number={3},
  pages={361--379},
  year={2011},
  publisher={Springer}
}
@article{chandak2021universal,
author = {Yash Chandak AND Scott Niekum AND Bruno Castro da Silva AND Erik Learned-Miller AND Emma Brunskill AND Philip S. Thomas},
title = {{Universal Off-Policy Evaluation}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2104.12820v2},
primaryClass = {cs.LG}}
@article{touati2021learning,
author = {Ahmed Touati AND Yann Ollivier},
title = {{Learning One Representation to Optimize All Rewards}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2103.07945v3},
primaryClass = {cs.LG}}
@article{denardo1967contraction,
  title={Contraction mappings in the theory underlying dynamic programming},
  author={Denardo, Eric V},
  journal={Siam Review},
  volume={9},
  number={2},
  pages={165--177},
  year={1967},
  publisher={SIAM}
}
@article{leonardos2021global,
author = {Stefanos Leonardos AND Will Overman AND Ioannis Panageas AND Georgios Piliouras},
title = {{Global Convergence of Multi-Agent Policy Gradient in Markov Potential
  Games}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2106.01969v3},
primaryClass = {cs.LG}
}
@article{eysenbach2021theinformation,
author = {Benjamin Eysenbach AND Ruslan Salakhutdinov AND Sergey Levine},
title = {{The Information Geometry of Unsupervised Reinforcement Learning}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2110.02719v1},
primaryClass = {cs.LG}}
@article{deac2021neural,
author = {Andreea Deac AND Petar Veličković AND Ognjen Milinković AND Pierre-Luc Bacon AND Jian Tang AND Mladen Nikolić},
title = {{Neural Algorithmic Reasoners are Implicit Planners}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2110.05442v1},
primaryClass = {cs.LG}}
@article{eysenbach2021maximum,
author = {Benjamin Eysenbach AND Sergey Levine},
title = {{Maximum Entropy RL (Provably) Solves Some Robust RL Problems}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2103.06257v1},
primaryClass = {cs.LG}}
@article{gobet2005sensitivity,
  title={Sensitivity Analysis Using It{\^o}--Malliavin Calculus and Martingales, and Application to Stochastic Optimal Control},
  author={Gobet, Emmanuel and Munos, R{\'e}mi},
  journal={SIAM Journal on control and optimization},
  volume={43},
  number={5},
  pages={1676--1713},
  year={2005},
  publisher={SIAM}
}
@inproceedings{antonoglou2022planning,
title={Planning in Stochastic Environments with a Learned Model},
author={Ioannis Antonoglou and Julian Schrittwieser and Sherjil Ozair and Thomas K Hubert and David Silver},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=X6D9bAHhBQ1}
}
@inproceedings{han2022desko,
title={De{SKO}: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator},
author={Minghao Han and Jacob Euler-Rolle and Robert K. Katzschmann},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=hniLRD_XCA}
}
@article{mahadevan07a,
  author  = {Sridhar Mahadevan and Mauro Maggioni},
  title   = {Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes},
  journal = {Journal of Machine Learning Research},
  year    = {2007},
  volume  = {8},
  number  = {74},
  pages   = {2169-2231},
  url     = {http://jmlr.org/papers/v8/mahadevan07a.html}
}
@article{hansen2019fast,
author = {Steven Hansen AND Will Dabney AND Andre Barreto AND Tom Van de Wiele AND David Warde-Farley AND Volodymyr Mnih},
title = {{Fast Task Inference with Variational Intrinsic Successor Features}},
year = {2019},
archivePrefix = {arXiv},
eprint = {1906.05030v2},
primaryClass = {cs.LG}}
@InProceedings{pmlr-v37-schaul15,
  title = 	 {Universal Value Function Approximators},
  author = 	 {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1312--1320},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schaul15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schaul15.html},
  abstract = 	 {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}
}
@article{borsa2018universal,
author = {Diana Borsa AND André Barreto AND John Quan AND Daniel Mankowitz AND Rémi Munos AND Hado van Hasselt AND David Silver AND Tom Schaul},
title = {{Universal Successor Features Approximators}},
year = {2018},
archivePrefix = {arXiv},
eprint = {1812.07626v1},
primaryClass = {cs.LG}}
@article{lan2022onthe,
author = {Charline Le Lan AND Stephen Tu AND Adam Oberman AND Rishabh Agarwal AND Marc G. Bellemare},
title = {{On the Generalization of Representations in Reinforcement Learning}},
year = {2022},
archivePrefix = {arXiv},
eprint = {2203.00543v1},
primaryClass = {cs.LG}}
@article{lhéritier2021acramér,
author = {Alix Lhéritier AND Nicolas Bondoux},
title = {{A Cramér Distance perspective on Quantile Regression based
  Distributional Reinforcement Learning}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2110.00535v2},
primaryClass = {stat.ML}}
@article{byravan2021evaluating,
author = {Arunkumar Byravan AND Leonard Hasenclever AND Piotr Trochim AND Mehdi Mirza AND Alessandro Davide Ialongo AND Yuval Tassa AND Jost Tobias Springenberg AND Abbas Abdolmaleki AND Nicolas Heess AND Josh Merel AND Martin Riedmiller},
title = {{Evaluating model-based planning and planner amortization for continuous
  control}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2110.03363v1},
primaryClass = {cs.RO}}
@article{daskalakis2021near-optimal,
author = {Constantinos Daskalakis AND Maxwell Fishelson AND Noah Golowich},
title = {{Near-Optimal No-Regret Learning in General Games}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2108.06924v1},
primaryClass = {cs.LG}}
@article{hartline2015no-regret,
author = {Jason Hartline AND Vasilis Syrgkanis AND Eva Tardos},
title = {{No-Regret Learning in Bayesian Games}},
year = {2015},
archivePrefix = {arXiv},
eprint = {1507.00418v2},
primaryClass = {cs.GT}}
@article{ferns2011bisimulation,
  title={Bisimulation metrics for continuous Markov decision processes},
  author={Ferns, Norm and Panangaden, Prakash and Precup, Doina},
  journal={SIAM Journal on Computing},
  volume={40},
  number={6},
  pages={1662--1714},
  year={2011},
  publisher={SIAM}
}
@article{odonoghue2018variational,
author = {Brendan O'Donoghue},
title = {{Variational Bayesian Reinforcement Learning with Regret Bounds}},
year = {2018},
archivePrefix = {arXiv},
eprint = {1807.09647v3},
primaryClass = {cs.LG}
}
@inproceedings{sriperumbudur2008injective,
  title={Injective Hilbert space embeddings of probability measures},
  author={Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Lanckriet, Gert and Sch{\"o}lkopf, Bernhard},
  booktitle={21st Annual Conference on Learning Theory (COLT 2008)},
  pages={111--122},
  year={2008},
  organization={Omnipress}
}
@article{yin2021efficient,
author = {Dong Yin AND Botao Hao AND Yasin Abbasi-Yadkori AND Nevena Lazić AND Csaba Szepesvári},
title = {{Efficient Local Planning with Linear Function Approximation}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2108.05533v3},
primaryClass = {cs.LG}}
@article{as2022constrained,
author = {Yarden As AND Ilnura Usmanova AND Sebastian Curi AND Andreas Krause},
title = {{Constrained Policy Optimization via Bayesian World Models}},
year = {2022},
archivePrefix = {arXiv},
eprint = {2201.09802v4},
primaryClass = {cs.LG}}
@article{rhinehart2021information,
author = {Nicholas Rhinehart AND Jenny Wang AND Glen Berseth AND John D. Co-Reyes AND Danijar Hafner AND Chelsea Finn AND Sergey Levine},
title = {{Information is Power: Intrinsic Control via Information Capture}},
year = {2021},
archivePrefix = {arXiv},
eprint = {2112.03899v1},
primaryClass = {cs.LG}}
@article{odonoghue2017theuncertainty,
author = {Brendan O'Donoghue AND Ian Osband AND Remi Munos AND Volodymyr Mnih},
title = {{The Uncertainty Bellman Equation and Exploration}},
year = {2017},
archivePrefix = {arXiv},
eprint = {1709.05380v4},
primaryClass = {cs.AI}}
@phdthesis{chow2017risk,
  title={Risk-sensitive and data-driven sequential decision making},
  author={Chow, Yinlam},
  year={2017},
  school={Stanford University}
}
@article{jiang2015risk-averse,
author = {Daniel R. Jiang AND Warren B. Powell},
title = {{Risk-Averse Approximate Dynamic Programming with Quantile-Based Risk
  Measures}},
year = {2015},
archivePrefix = {arXiv},
eprint = {1509.01920v4},
primaryClass = {math.OC}}
@article{howard1972risk,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/2629352},
 abstract = {This paper considers the maximization of certain equivalent reward generated by a Markov decision process with constant risk sensitivity. First, value iteration is used to optimize possibly time-varying processes of finite duration. Then a policy iteration procedure is developed to find the stationary policy with highest certain equivalent gain for the infinite duration case. A simple example demonstrates both procedures.},
 author = {Ronald A. Howard and James E. Matheson},
 journal = {Management Science},
 number = {7},
 pages = {356--369},
 publisher = {INFORMS},
 title = {Risk-Sensitive Markov Decision Processes},
 urldate = {2022-05-05},
 volume = {18},
 year = {1972}
}
@article{chung1987discounted,
  title={Discounted MDP’s: Distribution functions and exponential utility maximization},
  author={Chung, Kun-Jen and Sobel, Matthew J},
  journal={SIAM journal on control and optimization},
  volume={25},
  number={1},
  pages={49--62},
  year={1987},
  publisher={SIAM}
}
@article{busoniu2011optimistic,
author = {Lucian Busoniu AND Rémi Munos AND Bart de Schutter AND Robert Babuska},
title = {{Optimistic planning for sparsely stochastic systems}},
year = {2011},
archivePrefix = {arXiv},
eprint = {nil},
primaryClass = {nil}}
@online{finn2017model-agnostic,
author = {Chelsea Finn AND Pieter Abbeel AND Sergey Levine},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
year = {2017},
archivePrefix = {arXiv},
eprint = {1703.03400v3},
primaryClass = {cs.LG}}
@online{tamar2014optimizing,
author = {Aviv Tamar AND Yonatan Glassner AND Shie Mannor},
title = {{Optimizing the CVaR via Sampling}},
year = {2014},
archivePrefix = {arXiv},
eprint = {1404.3862v4},
primaryClass = {stat.ML}}
